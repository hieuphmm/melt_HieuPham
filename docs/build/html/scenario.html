<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Scenarios &mdash; MELTs 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=2709fde1"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Metrics" href="metrics.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            MELTs
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="instruction.html">How to use MELT?</a></li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metrics</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Scenarios</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#vieval.metrics.question_answering.QAMetric"><code class="docutils literal notranslate"><span class="pre">QAMetric</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vieval.metrics.question_answering.QAMetric.evaluate"><code class="docutils literal notranslate"><span class="pre">QAMetric.evaluate()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vieval.metrics.summary.SummaryMetric"><code class="docutils literal notranslate"><span class="pre">SummaryMetric</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vieval.metrics.summary.SummaryMetric.evaluate"><code class="docutils literal notranslate"><span class="pre">SummaryMetric.evaluate()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vieval.metrics.text_classification.TextClassificationMetric"><code class="docutils literal notranslate"><span class="pre">TextClassificationMetric</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vieval.metrics.text_classification.TextClassificationMetric.evaluate"><code class="docutils literal notranslate"><span class="pre">TextClassificationMetric.evaluate()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vieval.metrics.toxicity.ToxicityMetric"><code class="docutils literal notranslate"><span class="pre">ToxicityMetric</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vieval.metrics.toxicity.ToxicityMetric.evaluate"><code class="docutils literal notranslate"><span class="pre">ToxicityMetric.evaluate()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vieval.metrics.ir.InformationRetrievalMetric"><code class="docutils literal notranslate"><span class="pre">InformationRetrievalMetric</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vieval.metrics.ir.InformationRetrievalMetric.evaluate"><code class="docutils literal notranslate"><span class="pre">InformationRetrievalMetric.evaluate()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vieval.metrics.language.LanguageMetric"><code class="docutils literal notranslate"><span class="pre">LanguageMetric</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vieval.metrics.language.LanguageMetric.evaluate"><code class="docutils literal notranslate"><span class="pre">LanguageMetric.evaluate()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#vieval.metrics.language.LanguageMetric.get_num_bytes"><code class="docutils literal notranslate"><span class="pre">LanguageMetric.get_num_bytes()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vieval.metrics.reasoning.ReasoningMetric"><code class="docutils literal notranslate"><span class="pre">ReasoningMetric</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vieval.metrics.reasoning.ReasoningMetric.equal"><code class="docutils literal notranslate"><span class="pre">ReasoningMetric.equal()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#vieval.metrics.reasoning.ReasoningMetric.evaluate"><code class="docutils literal notranslate"><span class="pre">ReasoningMetric.evaluate()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#vieval.metrics.translation_metric.TranslationMetric"><code class="docutils literal notranslate"><span class="pre">TranslationMetric</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#vieval.metrics.translation_metric.TranslationMetric.evaluate"><code class="docutils literal notranslate"><span class="pre">TranslationMetric.evaluate()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MELTs</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Scenarios</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/scenario.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-vieval.metrics.question_answering">
<span id="scenarios"></span><h1>Scenarios<a class="headerlink" href="#module-vieval.metrics.question_answering" title="Link to this heading"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="vieval.metrics.question_answering.QAMetric">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">vieval.metrics.question_answering.</span></span><span class="sig-name descname"><span class="pre">QAMetric</span></span><a class="headerlink" href="#vieval.metrics.question_answering.QAMetric" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="metrics.html#vieval.metrics.base.BaseMetric" title="vieval.metrics.base.BaseMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseMetric</span></code></a></p>
<p>Evaluate the performance of a question-answering (QA) system.</p>
<dl class="py method">
<dt class="sig sig-object py" id="vieval.metrics.question_answering.QAMetric.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#vieval.metrics.question_answering.QAMetric.evaluate" title="Link to this definition"></a></dt>
<dd><p>Returns evaluation results for QA predictions.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>data (Dict): A dictionary expected to contain the keys “predictions” and “references”. It represents the dataset being evaluated, with “predictions” containing the model’s answers to the questions, and “references” containing the ground truth answers.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-vieval.metrics.summary">
<dt class="sig sig-object py" id="vieval.metrics.summary.SummaryMetric">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">vieval.metrics.summary.</span></span><span class="sig-name descname"><span class="pre">SummaryMetric</span></span><a class="headerlink" href="#vieval.metrics.summary.SummaryMetric" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="metrics.html#vieval.metrics.base.BaseMetric" title="vieval.metrics.base.BaseMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseMetric</span></code></a></p>
<p>Evaluate the quality of text summaries.</p>
<dl class="py method">
<dt class="sig sig-object py" id="vieval.metrics.summary.SummaryMetric.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#vieval.metrics.summary.SummaryMetric.evaluate" title="Link to this definition"></a></dt>
<dd><p>Evaluates the generated summaries against reference summaries and computes various metrics to assess the quality of the generated summaries.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>data (Dict): A dictionary expected to contain original_documents, predictions, and references as keys.</p>
</dd>
<dt>Returns:</dt><dd><p>Returns a tuple containing the original data dictionary and the result dictionary with all the computed metrics.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-vieval.metrics.text_classification">
<dt class="sig sig-object py" id="vieval.metrics.text_classification.TextClassificationMetric">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">vieval.metrics.text_classification.</span></span><span class="sig-name descname"><span class="pre">TextClassificationMetric</span></span><a class="headerlink" href="#vieval.metrics.text_classification.TextClassificationMetric" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="metrics.html#vieval.metrics.base.BaseMetric" title="vieval.metrics.base.BaseMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseMetric</span></code></a></p>
<p>Evaluate text classification models.</p>
<dl class="py method">
<dt class="sig sig-object py" id="vieval.metrics.text_classification.TextClassificationMetric.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#vieval.metrics.text_classification.TextClassificationMetric.evaluate" title="Link to this definition"></a></dt>
<dd><p>Evaluates the classification performance given the predictions, references, and additional arguments.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>data (Dict): A dictionary expected to contain keys like predictions, references, and option_probs.</p>
</dd>
<dt>Returns:</dt><dd><p>Returns a tuple containing the original data dictionary and the result dictionary with all the computed metrics.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-vieval.metrics.toxicity">
<dt class="sig sig-object py" id="vieval.metrics.toxicity.ToxicityMetric">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">vieval.metrics.toxicity.</span></span><span class="sig-name descname"><span class="pre">ToxicityMetric</span></span><a class="headerlink" href="#vieval.metrics.toxicity.ToxicityMetric" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="metrics.html#vieval.metrics.base.BaseMetric" title="vieval.metrics.base.BaseMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseMetric</span></code></a></p>
<p>Evaluate text for toxicity.</p>
<dl class="py method">
<dt class="sig sig-object py" id="vieval.metrics.toxicity.ToxicityMetric.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#vieval.metrics.toxicity.ToxicityMetric.evaluate" title="Link to this definition"></a></dt>
<dd><p>Evaluates the level of toxicity in the text predictions provided via the dictionary.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>data (Dict): A dictionary expected to contain a key “predictions” with text data that needs to be evaluated for toxicity.</p>
</dd>
<dt>Returns:</dt><dd><p>Returns a tuple containing the updated data dictionary and a new dictionary with the mean toxicity score calculated from the toxicity scores list.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-vieval.metrics.ir">
<dt class="sig sig-object py" id="vieval.metrics.ir.InformationRetrievalMetric">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">vieval.metrics.ir.</span></span><span class="sig-name descname"><span class="pre">InformationRetrievalMetric</span></span><a class="headerlink" href="#vieval.metrics.ir.InformationRetrievalMetric" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="metrics.html#vieval.metrics.base.BaseMetric" title="vieval.metrics.base.BaseMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseMetric</span></code></a></p>
<p>Evaluate information retrieval systems.</p>
<dl class="py method">
<dt class="sig sig-object py" id="vieval.metrics.ir.InformationRetrievalMetric.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#vieval.metrics.ir.InformationRetrievalMetric.evaluate" title="Link to this definition"></a></dt>
<dd><p>Evaluates the predictions using relevance judgments and computes various metrics.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>data (Dict): A dictionary containing predictions to be evaluated.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-vieval.metrics.language">
<dt class="sig sig-object py" id="vieval.metrics.language.LanguageMetric">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">vieval.metrics.language.</span></span><span class="sig-name descname"><span class="pre">LanguageMetric</span></span><a class="headerlink" href="#vieval.metrics.language.LanguageMetric" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="metrics.html#vieval.metrics.base.BaseMetric" title="vieval.metrics.base.BaseMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseMetric</span></code></a></p>
<p>Evaluate language generation tasks.</p>
<dl class="py method">
<dt class="sig sig-object py" id="vieval.metrics.language.LanguageMetric.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#vieval.metrics.language.LanguageMetric.evaluate" title="Link to this definition"></a></dt>
<dd><p>Evaluates the predictions against references and computes various metrics.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>data (Dict): A dictionary that must contain keys “predictions”, “references”, and “generation_probs”. It is used to store the predictions, the references for comparison, and the log probabilities for each prediction.</p>
</dd>
<dt>Returns:</dt><dd><p>Returns a tuple containing:
- data: The original data dictionary, updated with raw metric scores for each prediction-reference pair.
- result: A dictionary with the average scores of the metrics across all prediction-reference pairs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="vieval.metrics.language.LanguageMetric.get_num_bytes">
<span class="sig-name descname"><span class="pre">get_num_bytes</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#vieval.metrics.language.LanguageMetric.get_num_bytes" title="Link to this definition"></a></dt>
<dd><p>Calculates the total number of bytes of a list of tokens when encoded in UTF-8.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>tokens (List[str]): A list of string tokens for which the byte length is to be calculated.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-vieval.metrics.reasoning">
<dt class="sig sig-object py" id="vieval.metrics.reasoning.ReasoningMetric">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">vieval.metrics.reasoning.</span></span><span class="sig-name descname"><span class="pre">ReasoningMetric</span></span><a class="headerlink" href="#vieval.metrics.reasoning.ReasoningMetric" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="metrics.html#vieval.metrics.base.BaseMetric" title="vieval.metrics.base.BaseMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseMetric</span></code></a></p>
<p>Evaluate reasoning capabilities, particularly in scenarios that may involve mathematical reasoning.</p>
<dl class="py method">
<dt class="sig sig-object py" id="vieval.metrics.reasoning.ReasoningMetric.equal">
<span class="sig-name descname"><span class="pre">equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prediction</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">refenrence</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.9</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#vieval.metrics.reasoning.ReasoningMetric.equal" title="Link to this definition"></a></dt>
<dd><p>Evaluates whether the prediction is sufficiently close to the refenrence using a similarity threshold. It employs the Levenshtein ratio (a measure of the similarity between two strings) and returns 1 if the ratio exceeds the threshold, indicating a match, otherwise 0.</p>
<dl>
<dt>Args:</dt><dd><p>prediction (str): The predicted answer.</p>
<p>refenrence (str): The reference or ground truth answer.</p>
<p>threshold (int, optional): A similarity threshold for comparing the prediction and reference, defaulting to 0.9.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="vieval.metrics.reasoning.ReasoningMetric.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#vieval.metrics.reasoning.ReasoningMetric.evaluate" title="Link to this definition"></a></dt>
<dd><p>Evaluates predictions against references contained within the dictionary using various metrics.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>data (Dict): A dictionary that must contain the keys “predictions” and “references”.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-vieval.metrics.translation_metric">
<dt class="sig sig-object py" id="vieval.metrics.translation_metric.TranslationMetric">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">vieval.metrics.translation_metric.</span></span><span class="sig-name descname"><span class="pre">TranslationMetric</span></span><a class="headerlink" href="#vieval.metrics.translation_metric.TranslationMetric" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="metrics.html#vieval.metrics.base.BaseMetric" title="vieval.metrics.base.BaseMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseMetric</span></code></a></p>
<p>Evaluate the quality of text translations using metrics like BLEU and hLepor.</p>
<dl class="py method">
<dt class="sig sig-object py" id="vieval.metrics.translation_metric.TranslationMetric.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#vieval.metrics.translation_metric.TranslationMetric.evaluate" title="Link to this definition"></a></dt>
<dd><p>Computes the translation quality metrics for a set of predictions and references provided in the dictionary.</p>
<dl>
<dt>Args:</dt><dd><p>data (Dict): A dictionary expected to contain two keys:</p>
<blockquote>
<div><ul class="simple">
<li><p>predictions: A list of translated texts generated by the translation model.</p></li>
<li><p>references: A list of reference translations for evaluating the quality of the model’s predictions.</p></li>
</ul>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><ol class="arabic">
<li><p>The original data dictionary, which contains the raw predictions and references.</p></li>
<li><p>A result dictionary with the following keys:</p>
<blockquote>
<div><ul class="simple">
<li><p>“bleu”: The computed BLEU score for the translations.</p></li>
<li><p>“hLepor”: The computed hLepor score for the translations.</p></li>
</ul>
</div></blockquote>
</li>
</ol>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="metrics.html" class="btn btn-neutral float-left" title="Metrics" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Thu Nguyen Hoang Anh.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>